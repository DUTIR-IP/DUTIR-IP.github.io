---
layout: page
---

## IPEval Benchmark
<a href="javascript:window.location='https://huggingface.co/datasets/Mathsion/IPEval';">![Data](https://img.shields.io/badge/IPEval-Data-{brightgreen})</a>
<a href="javascript:window.location='https://arxiv.org/abs/2406.12386';">![Paper](https://img.shields.io/badge/IPEval-Paper-{red})</a>
<a href="javascript:window.location='https://github.com/Mathsion2/IPEval';">![Github](https://img.shields.io/badge/IPEval-Github-{red})</a>


---
### About IPEval
IPEval is a pioneering bilingual Intellectual Property (IP) agency consultation evaluation benchmark, meticulously crafted to assess the competencies of Large Language Models (LLMs) in the intricate domain of intellectual property. This benchmark is the first of its kind, encompassing a diverse spectrum of 2,657 multiple-choice questions that are intricately divided across four major capability dimensions: creation, application, protection, and management. More details can be found in our paper.

---
### Data
Our data can be directly downloaded on <a href="https://huggingface.co/datasets/Mathsion/IPEval">Huggingface datasets</a>. Please refer to our <a href="https://github.com/Mathsion2/IPEval"> github instructions </a> for how to read and use the data.

---
### Citation
```
We will upload the paper to arXiv or make supplementary actions after it is accepted.
```

---
### Contact Us
Have any questions about IPEval? Please contact us at wangqiyao@mail.dlut.edu.cn or create an issue on out github.
